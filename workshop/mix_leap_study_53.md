# Mix Leap Study #53 - Kubernetesで課題解決（中継開催）

## ヤフーのナビゲーション系のバックエンドサービスの課題をk8sで解決した話
* ヤフーのナビゲーション機能
  * マップ、カーナビ、乗換案内

* 従来のアーキテクチャ
  * アプリサーバ、Webサーバ、YOLP API、オブジェクトストレージ等
  * サーバはVM

* 抱えていた課題
  * IFが違うだけでロジックはほぼ同じだった
  * データ更新のシェルは半日かかっていた
  * 脆弱性対応にリソースを割かれ、新規開発のリソースが無かった
  * ヒューマンエラー
  * 障害時の手動オペ
  * 自動でスケールしない

* 改善後のアーキテクチャ
  * API
  * アプリとWebを統合
  * PaaSに移行

* エンジン
  * CaaSに移行
  * バイナリデータはInit Containerで取得後、オンメモリに格納

* データを入稿したらk8sのCron実行にした

* 改善により解決されたこと
  * 検証範囲が減った
  * 保守コストが減った
  * スパイクにスケールするようになった
  * バイナリデータ生成の作業コストが減った

* CI/CDはScrewDriverというOSS
  * https://screwdriver.cd/

## プライベートクラウド刷新中に感じているk8sエコシステムの良さみ
* サイボウズのインフラ
  * 自社製プライベートクラウド、OpenStackは使ってない
  * ガタがきててつらいので２０１８年に刷新プロジェクトが始まった

* インフラ刷新の進捗
  * ３つのDCでk8sクラスタが稼働中
  * 管理ツールとしてCKEがある
  * Rook/Cephもある
  * サービス移行プロジェクトとしてManekiというプロジェクトをやっている

* 刷新プロジェクトの目的
  * 運用コスト低減
  * スケーラビリティ向上

* 運用コストは下がった？
  * 旧環境
  * Ubuntuでコンテナは不使用
  * OSアップグレードするだけで大変
  * サービス停止時は人力が多く大変

* 新環境
  * CoreOSを採用
  * 内蔵ミドルウェアが少ない
  * ネットブートにかかる時間も少ない
  * OSブートストラップやアップグレードを自動テスト化
  * 毎日CI試験を回して常にOSアップグレードできるようにした
  * OSアップグレードはdrain、再起動、uncordon

* サービスデプロイ
  * 旧環境
  * 手動で大変
  * DevとOpsが完全に分離していた

* 新環境
  * Yamlにお任せ
  * 必要に応じてカスタムコントローラを導入
  * CI/CDはGitOps、ArgoCD、Kustomize
  * 各チームに適切な権限を割り振ることでk8sリソースを気軽に触れるようにした
  * RBAC、Admission Controller、NetworkPolicyなどを利用

* 開発環境
  * 旧環境
  * 本番と同型のクラスタを共同利用していた
  * 迷惑をかけやすい

* 新環境
  * Minikube、microk8s、Kind、GKEなどを用意
  * 結局Kindを採用してミドルウェアを動かすようにカスタマイズした

* その他
  * 証明書発行を自動化
  * Contour、Cert-manager、External DNS、Contour-plus
  * カスタムリソース作成、CertとDNSEndpointが作成される、TLS証明書とAレコードが自動作成される
  * Teleportという踏み台サーバが使えるようになった
  * SSOでGithubTeamをSSO権限制御できる、ターミナルの入出力を録画できる

## PFNにおける二種類のKubernetesクラスタ
* 自作GPUクラスタとWebAppクラスタについて
  * 機械学習を使った事業がメイン
  * 機械学習を使ったサービスは自社クラスタ
  * WebAppクラスタはEKSを使っている

* KubernetesのNetwork Isolationについて
  * 一つのEKSクラスタに複数社の関係ないアプリが同居する
  * 相互通信してはいけないのでNetwork Isolationが重要となる
  * それは同一顧客でも同様である
  * Network Policiesという標準機能がある
  * PodやNamespace単位で通信を制御できる
  * 一顧客につき一Namespace
  * Kubernetesはデフォルトで相互通信ができる
  * 明示的に禁止する必要がある
  * Namespaceを作るたびにこの設定をしたくない
  * Calicoというソフトウェアを使った
  * Global Network policyという設定でデフォルトルールとして使える
  * Kube-systemへの許可が必要なので、これだけはあらゆる通信を許可している
  * IngressとAWSの相性問題
  * アクセスはVPCの外からくる
  * ALBをたてEKSへアクセス、ALB ingress controllerを利用
  * ただしNetwork PoliciesはALBを対象にできない
  * Subnet構成でALBとEKSを分けて回避している

## k8sで画像PFを一年半運用してみた振り返り
* 画像プロキシサーバ
  * サムネイル自動生成などのサービス

* 移行までの経緯
  * VMで動く古いシステムがあった

* 失敗体験
  * 移行前の構成を真似してもってきてしまった
  * モノリシックな部分だけマイクロサービスっぽくした
  * Podの配置に困った
  * Podの中に全てのコンテナをまとめてしまった
  * 起動が遅かったりヘルスチェックが複雑だったりYAMLが肥大化した
  * 一Pod一Containerに直した
  * 鍵はVolume内で共有

* 監視もよくわからない
  * 移行前の構成の監視項目をPrometheusに移した
  * 監視項目が多すぎてアラートがくる
  * アラートの必要性がわからなかった

* 独自ツールを運用に持ち込んだ
  * デプロイ属人化
  * KustomizeやHelmを使うことで改善

* SIGTERMのハンドリングはやってよかった

## k8s初心者がgRPCとenvoyを導入したら色々苦労した話
* システム概要
  * ヤフーショッピングではカート、検索、ランキングなどチームがある
  * サービス単位ではモノリスな物が多い
  * 順にリプレイスをしている
  * ページをまたいだ共通コンポーネントは動的なロジックが多い

* 実際の構成
  * サービスはフロントとドメインの２つに分けた

* 不具合について
  * ドメインサービスでローリングアップデートを実行した
  * フロントサービスでコネクションエラーが継続的に発生した

* 解決のプロセス
  * ServiceのDNSから外れているがリクエスト済みのセッションが残っているのでは？
  * GracefulShutdown
  * 検証として低負荷状態でPodをスケールさせ、Pod単位の負荷を下げてみた
  * 検証結果としてスタックは原因でなかった

* envoyがIPTableを更新する前に存在しないPodへリクエストが送られた？
  * PreStopを１秒から３０秒に変更した
  * これも原因ではなかった

* gRPCのコネクション管理に問題ないか確認した
  * すでに使われているIPリストが残っているのでは？
  * Zipkinを導入してTraceIdから失敗したIDを照合した
  * ログに失敗したTraceIdがなかったのでenvoyの中に古いgRPCリクエストが残っている
  * grpc_health_checkを設定すれば使えなくなったIPをコネクションから外すようにする
  * 自殺するためのエントリーポイントを作成してSIGKILLを受け取ると実行するようにした

